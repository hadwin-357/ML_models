{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbue4SQgeFP8ay3wO4TVZj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hadwin-357/ML_models/blob/main/custom_implement_SVM_through_Lagrange.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing dual problem of SVM (Lagrange primal problem):\n",
        "\n",
        "## The following theory taken from https://towardsdatascience.com/support-vector-machines-learning-data-science-step-by-step-f2a569d90f76"
      ],
      "metadata": {
        "id": "6Y22BQsbcXjr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following explanation is about the binary classification but generalizes to more classes.\n",
        "\n",
        "Let $X$ be the matrix of $n$ samples of the $p$ features. We want to separate the two classes of $y$ with an hyperplan (a straight line in 2D, that is $p=2$). The separation equation is:\n",
        "\n",
        "$$ w^T x + b = 0, w \\in \\mathbb{R}^{p}, x \\in \\mathbb{R}^{p}, b \\in \\mathbb{R} $$\n",
        "\n",
        "Given $x_0$ a point on the hyperplan, the signed distance of any point $x$ to the hyperplan is :\n",
        "$$ \\frac{w}{\\Vert w \\Vert} (x - x_0) = \\frac{1}{\\Vert w \\Vert} (w^T x + b) $$\n",
        "\n",
        "If $y$, such that $y \\in \\{-1, 1\\}$, is the corresponding label of $x$, the (unsigned) distance is :\n",
        "$$ \\frac{y}{\\Vert w \\Vert} (w^T x + b) $$\n",
        "\n",
        "This is the update quantity used by the Rosenblatt Perceptron."
      ],
      "metadata": {
        "id": "tXbrbM6tc4Qo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The __Maximum margin separator__ is aiming at maximizing $M$ such that :\n",
        "$$ \\underset{w, b}{\\max} M $$\n",
        "__Subject to :__\n",
        "- $y_i(x_i^T w + b) \\ge M, i = 1..n$\n",
        "- $\\Vert w \\Vert = 1$\n",
        "\n",
        "$x_i$ and $y_i$ are samples of $x$ and $y$, a row of the matrix $X$ and the vector $y$.\n",
        "\n",
        "However, we may change the condition on the norm of $w$ such that : $\\Vert w \\Vert = \\frac 1M$\n",
        "\n",
        "Leading to the equivalent statement of the maximum margin classifier :\n",
        "$$ \\min_{w, b} \\frac 12 \\Vert w \\Vert^2 $$\n",
        "__Subject to : $y_i(x_i^T w + b) \\ge 1, i = 1..n$__"
      ],
      "metadata": {
        "id": "ysSPEIPbc2Qe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The corresponding Lagrange primal problem is :\n",
        "\n",
        "$$\\mathcal{L}_p(w, b, \\alpha) = \\frac 12 \\Vert w \\Vert^2 - \\sum_{i=0}^n \\alpha_i (y_i(x_i^T w + b) - 1)$$\n",
        "\n",
        "__Subject to:__\n",
        "- $\\alpha_i \\ge 0, i\\in 1..n$\n",
        "\n",
        "This shall be __minimized__ on $w$ and $b$, using the corresponding partial derivates equal to 0, we get :\n",
        "$$\\begin{align}\n",
        "\\sum_{i=0}^n \\alpha_i y_i x_i &= w \\\\\n",
        "\\sum_{i=0}^n \\alpha_i y_i &= 0\n",
        "\\end{align}$$\n",
        "\n",
        "From $\\mathcal{L}_p$, we get the (Wolfe) dual :\n",
        "$$\\begin{align}\n",
        "\\mathcal{L}_d (\\alpha)\n",
        "&= \\sum_{i=0}^n \\alpha_i - \\frac 12 \\sum_{i=0}^n \\sum_{k=0}^n \\alpha_i \\alpha_k y_i y_k x_i^T x_k \\\\\n",
        "&= \\sum_{i=0}^n \\alpha_i - \\frac 12 \\sum_{i=0}^n \\sum_{k=0}^n \\langle \\alpha_i y_i x_i, \\alpha_k y_k  x_k \\rangle \\\\\n",
        "\\end{align}$$\n",
        "\n",
        "__Subject to :__\n",
        "- $\\alpha_i \\ge 0, i\\in 1..n$\n",
        "- $\\sum_{i=0}^n \\alpha_i y_i = 0$\n",
        "\n",
        "Which is a concave problem that is __maximized__ using a solver.\n",
        "\n",
        "Strong duality requires (KKT) [2, chap. 5.5]:\n",
        "- $\\alpha_i (y_i(x_i^T w + b) - 1) = 0,  \\forall i \\in 1..n$\n",
        "\n",
        "Implying that :\n",
        "- If $\\alpha_i > 0$, then $y_i(x_i^T w + b) = 1$, meaning that $x_i$ is on one of the two hyperplans located at the margin distance from the separating hyperplan. $x_i$ is said to be a support vector\n",
        "- If $y_i(x_i^T w + b) > 1$, the distance of $x_i$ to the hyperplan is larger than the margin."
      ],
      "metadata": {
        "id": "UgT3mOpBdC18"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\\mathcal{L}_d = \\sum_{i=0}^n \\alpha_i - \\frac 12 \\sum_{i=0}^n \\sum_{k=0}^n \\alpha_i \\alpha_k y_i y_k x_i^T x_k $$\n",
        "\n",
        "__Subject to :__\n",
        "- $\\sum_{i=0}^n \\alpha_i y_i = \\langle \\alpha, y \\rangle = 0$\n",
        "- $\\alpha_i \\ge 0, i\\in 1..n$\n",
        "\n",
        "The classifier is built on the scipy.optimize.minimum solver. The implementation is correct but inefficient as it is not taking into account for the sparsity of the $\\alpha$ vector."
      ],
      "metadata": {
        "id": "8SRI_ESvdLyX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import optimize\n",
        "class SVM_lagrange():\n",
        "  def __init__(self):\n",
        "        self.alpha = None\n",
        "        self.w = None\n",
        "        self.supportVectors = None\n",
        "\n",
        "  def fit(self, X, y):\n",
        "      N = len(y)\n",
        "      # Gram matrix of (X.y)\n",
        "      Xy = X * y[:, np.newaxis]\n",
        "      GramXy = np.matmul(Xy, Xy.T)\n",
        "\n",
        "      # Lagrange dual problem\n",
        "      def Ld0(G, alpha):\n",
        "          return alpha.sum() - 0.5 * alpha.dot(alpha.dot(G))\n",
        "\n",
        "      # Partial derivate of Ld on alpha\n",
        "      def Ld0dAlpha(G, alpha):\n",
        "          return np.ones_like(alpha) - alpha.dot(G)\n",
        "\n",
        "      # Constraints on alpha of the shape :\n",
        "      # -  d - C*alpha  = 0\n",
        "      # -  b - A*alpha >= 0\n",
        "      A = -np.eye(N)\n",
        "      b = np.zeros(N)\n",
        "      constraints = ({'type': 'eq',   'fun': lambda a: np.dot(a, y), 'jac': lambda a: y},\n",
        "                      {'type': 'ineq', 'fun': lambda a: b - np.dot(A, a), 'jac': lambda a: -A})\n",
        "\n",
        "      # Maximize by minimizing the opposite\n",
        "      optRes = optimize.minimize(fun=lambda a: -Ld0(GramXy, a),\n",
        "                                  x0=np.ones(N),\n",
        "                                  method='SLSQP',\n",
        "                                  jac=lambda a: -Ld0dAlpha(GramXy, a),\n",
        "                                  constraints=constraints)\n",
        "      self.alpha = optRes.x\n",
        "      self.w = np.sum((self.alpha[:, np.newaxis] * Xy), axis=0)\n",
        "      epsilon = 1e-6\n",
        "      self.supportVectors = X[self.alpha > epsilon]\n",
        "      # Any support vector is at a distance of 1 to the separation plan\n",
        "      # => use support vector #0 to compute the intercept, assume label is in {-1, 1}\n",
        "      supportLabels = y[self.alpha > epsilon]\n",
        "      self.intercept = supportLabels[0] - np.matmul(self.supportVectors[0].T, self.w)\n",
        "\n",
        "  def predict(self, X):\n",
        "      \"\"\" Predict y value in {-1, 1} \"\"\"\n",
        "      assert(self.w is not None)\n",
        "      assert(self.w.shape[0] == X.shape[1])\n",
        "      return 2 * (np.matmul(X, self.w) > 0) - 1"
      ],
      "metadata": {
        "id": "CCYfDz-ldIBe"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from timeit import default_timer as timer\n",
        "def print_train_time(start: float, end: float):\n",
        "    \"\"\"Prints difference between start and end time.\n",
        "\n",
        "    Args:\n",
        "        start (float): Start time of computation (preferred in timeit format).\n",
        "        end (float): End time of computation.\n",
        "        device ([type], optional): Device that compute is running on. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        float: time between start and end in seconds (higher is longer).\n",
        "    \"\"\"\n",
        "    total_time = end - start\n",
        "    print(f\"Train time : {total_time:.3f} seconds\")\n",
        "    return total_time"
      ],
      "metadata": {
        "id": "g9Xjl1_vddp1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "import numpy as np\n",
        "cancer = datasets.load_breast_cancer()\n",
        "#convert label 0 to -1 for SVM\n",
        "y= np.where(cancer['target']==0, -1, 1)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "        cancer['data'], y, test_size=0.2, random_state=42\n",
        "    )"
      ],
      "metadata": {
        "id": "daPrt05vdlU_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time=timer()\n",
        "model_1 =SVM_lagrange()\n",
        "model_1.fit(X_train,y_train)\n",
        "y_pred =model_1.predict(X_test)\n",
        "\n",
        "end_time =timer()\n",
        "\n",
        "print(f'accuracy score: {accuracy_score(y_test, y_pred)}')\n",
        "print(f\"confusion matrix:{confusion_matrix(y_test,y_pred)}\")\n",
        "print_train_time(start_time, end_time)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_C8kgugQdXIK",
        "outputId": "80e391c0-6f6c-45d7-b0ba-064ed6f80cd2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy score: 0.37719298245614036\n",
            "confusion matrix:[[43  0]\n",
            " [71  0]]\n",
            "Train time : 30.897 seconds\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30.896577390000004"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Conclusion\n",
        "\n",
        "\n",
        "1.  The predict seems quite wrong, further troubleshoot is needed\n",
        "2.  Using inv of matrix has O(N^3) in computating complexity, which significantly slow down the program."
      ],
      "metadata": {
        "id": "tqi6a8u_eDja"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wNywXKF8dbUn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}